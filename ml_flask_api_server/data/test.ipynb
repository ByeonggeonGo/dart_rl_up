{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Blueprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import copy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import requests\n",
    "import pandas_datareader.data as web\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import Series\n",
    "from numpy import nan\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "# from flask import jsonify/\n",
    "\n",
    "path = os.getcwd()\n",
    "csv_list = sorted(glob(os.path.join(path,\"DB\",\"*.csv\")))\n",
    "\n",
    "data = pd.read_csv(csv_list[1])\n",
    "data.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_51 (InputLayer)        [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_329 (Dense)            (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_330 (Dense)            (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_331 (Dense)            (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_332 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_333 (Dense)            (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_334 (Dense)            (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 1,419\n",
      "Trainable params: 1,419\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from flask import Blueprint\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "def train(model, x, y):\n",
    "  with tf.GradientTape() as t:\n",
    "    # Trainable variables are automatically tracked by GradientTape\n",
    "    current_loss = loss(y_true=y,y_pred=model(x, training=True))\n",
    "\n",
    "  # Use GradientTape to calculate the gradients with respect to W and b\n",
    "  grads= t.gradient(current_loss, model.trainable_variables)\n",
    "  \n",
    "  # Subtract the gradient scaled by the learning rate\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "def training_loop(model, x, y, validset = None):\n",
    "  for epoch in epochs:\n",
    "    # Update the model with the single giant batch\n",
    "    train(model, x, y)\n",
    "\n",
    "    # Track this before I update\n",
    "    if validset == None:\n",
    "        current_loss = loss(y,model(x))\n",
    "        print(\"Epoch %2d: loss=%2.5f\" %\n",
    "            (epoch, current_loss))\n",
    "    else:\n",
    "        x_val = validset[0]\n",
    "        y_val = validset[1]\n",
    "\n",
    "        current_loss = loss(y_true=y,y_pred=model(x))\n",
    "        valid_loss = loss(y_true=y_val,y_pred=model(x_val))\n",
    "        print(\"Epoch %2d: loss=%2.5f  val_loss=%2.5f\" %\n",
    "            (epoch, current_loss, valid_loss))\n",
    "\n",
    "#데이터 셋\n",
    "path = os.getcwd()\n",
    "csv_list = sorted(glob(os.path.join(path,\"DB\",\"*.csv\")))\n",
    "data = pd.read_csv(csv_list[1])\n",
    "trainset, testset = train_test_split(data,test_size=0.2,shuffle=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = trainset.values[:,:-1]\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "y_train = trainset.values[:,-1]\n",
    "# y_train = pd.get_dummies(trainset.values[:,-1]).values\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "\n",
    "\n",
    "X_test = testset.values[:,:-1]\n",
    "X_test = scaler.transform(X_test)\n",
    "y_test = testset.values[:,-1]\n",
    "# y_test = pd.get_dummies(testset.values[:,-1]).values\n",
    "y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "\n",
    "\n",
    "#params\n",
    "input_shape = 4\n",
    "output_shape = 3\n",
    "lr = 0.03\n",
    "act_function = 'relu'\n",
    "outlayer_act_function = 'softmax'\n",
    "category = \"classification\"\n",
    "#optimizer\n",
    "optimizer = keras.optimizers.SGD(learning_rate=lr)\n",
    "\n",
    "# #loss function\n",
    "loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "num_perceptron_layer1 = 8\n",
    "num_perceptron_layer2 = 16\n",
    "num_perceptron_layer3 = 32\n",
    "num_perceptron_layer4 = 16\n",
    "num_perceptron_layer5 = 8\n",
    "num_perceptron_out = output_shape\n",
    "\n",
    "num_perceptron_list = [\n",
    "    num_perceptron_layer1,\n",
    "    num_perceptron_layer2,\n",
    "    num_perceptron_layer3,\n",
    "    num_perceptron_layer4,\n",
    "    num_perceptron_layer5,\n",
    "    num_perceptron_out,\n",
    "    ]\n",
    "\n",
    "# model definition\n",
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "for i in range(len(num_perceptron_list)):\n",
    "    n_percep = num_perceptron_list[i]\n",
    "    if i == 0:\n",
    "        x = keras.layers.Dense(n_percep, activation= act_function)(inputs)\n",
    "\n",
    "    elif  i != 0 and n_percep != 0 and i != len(num_perceptron_list)-1:\n",
    "        x = keras.layers.Dense(n_percep, activation= act_function)(x)\n",
    "\n",
    "    elif  i == len(num_perceptron_list)-1:\n",
    "        x = keras.layers.Dense(n_percep,)(x)#activation= outlayer_act_function)(x)\n",
    "\n",
    "    else: pass\n",
    "\n",
    "dense_model = tf.keras.Model(name=\"model_1\",inputs=inputs, outputs=x)\n",
    "dense_model.summary()\n",
    "\n",
    "\n",
    "# Define a training loop\n",
    "epochs = range(300)    \n",
    "# print(y_train[:5],dense_model(X_train[:5]))\n",
    "# print(loss(y_train[:5],dense_model(X_train[:5])))\n",
    "# print(loss(y_train[:5], dense_model(X_train[:5])))\n",
    "# Do the training\n",
    "training_loop(dense_model, X_train, y_train,validset=[X_test,y_test])\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "y_pred_test = dense_model(X_test).numpy()\n",
    "y_pred_test = np.argmax(y_pred_test, axis=1).tolist()\n",
    "# y_test = np.argmax(y_test, axis=1).tolist()\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "# print(metrics.classification_report(y_test, y_pred_test, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    # Trainable variables are automatically tracked by GradientTape\n",
    "    current_loss = loss(y_true=y_train,y_pred=dense_model(X_train))\n",
    "\n",
    "# Use GradientTape to calculate the gradients with respect to W and b\n",
    "grads= t.gradient(current_loss, dense_model.trainable_variables)\n",
    "# print(grads)\n",
    "# print(dense_model.trainable_variables)\n",
    "# print('ok')\n",
    "# optimizer.apply_gradients(zip(grads,dense_model.trainable_variables))\n",
    "# print(optimizer.apply_gradients(zip(grads,dense_model.trainable_variables)))\n",
    "# print(current_loss)\n",
    "# print(optimizer.iterations.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0: loss=1.08465  val_loss=1.08620\n",
      "Epoch  1: loss=1.07699  val_loss=1.07819\n",
      "Epoch  2: loss=1.07047  val_loss=1.07135\n",
      "Epoch  3: loss=1.06500  val_loss=1.06565\n",
      "Epoch  4: loss=1.06111  val_loss=1.06165\n",
      "Epoch  5: loss=1.05775  val_loss=1.05838\n",
      "Epoch  6: loss=1.05462  val_loss=1.05536\n",
      "Epoch  7: loss=1.05170  val_loss=1.05220\n",
      "Epoch  8: loss=1.04872  val_loss=1.04927\n",
      "Epoch  9: loss=1.04572  val_loss=1.04611\n",
      "Epoch 10: loss=1.04276  val_loss=1.04318\n",
      "Epoch 11: loss=1.03986  val_loss=1.04017\n",
      "Epoch 12: loss=1.03692  val_loss=1.03718\n",
      "Epoch 13: loss=1.03395  val_loss=1.03412\n",
      "Epoch 14: loss=1.03093  val_loss=1.03107\n",
      "Epoch 15: loss=1.02785  val_loss=1.02790\n",
      "Epoch 16: loss=1.02472  val_loss=1.02469\n",
      "Epoch 17: loss=1.02156  val_loss=1.02133\n",
      "Epoch 18: loss=1.01831  val_loss=1.01803\n",
      "Epoch 19: loss=1.01500  val_loss=1.01448\n",
      "Epoch 20: loss=1.01155  val_loss=1.01094\n",
      "Epoch 21: loss=1.00773  val_loss=1.00709\n",
      "Epoch 22: loss=1.00145  val_loss=1.00061\n",
      "Epoch 23: loss=0.99573  val_loss=0.99489\n",
      "Epoch 24: loss=0.99059  val_loss=0.98966\n",
      "Epoch 25: loss=0.98537  val_loss=0.98437\n",
      "Epoch 26: loss=0.98006  val_loss=0.97898\n",
      "Epoch 27: loss=0.97462  val_loss=0.97348\n",
      "Epoch 28: loss=0.96900  val_loss=0.96787\n",
      "Epoch 29: loss=0.96321  val_loss=0.96205\n",
      "Epoch 30: loss=0.95729  val_loss=0.95602\n",
      "Epoch 31: loss=0.95127  val_loss=0.94988\n",
      "Epoch 32: loss=0.94514  val_loss=0.94367\n",
      "Epoch 33: loss=0.93891  val_loss=0.93735\n",
      "Epoch 34: loss=0.93259  val_loss=0.93100\n",
      "Epoch 35: loss=0.92618  val_loss=0.92451\n",
      "Epoch 36: loss=0.91965  val_loss=0.91794\n",
      "Epoch 37: loss=0.91300  val_loss=0.91112\n",
      "Epoch 38: loss=0.90633  val_loss=0.90446\n",
      "Epoch 39: loss=0.89937  val_loss=0.89725\n",
      "Epoch 40: loss=0.89235  val_loss=0.89024\n",
      "Epoch 41: loss=0.88523  val_loss=0.88287\n",
      "Epoch 42: loss=0.87797  val_loss=0.87562\n",
      "Epoch 43: loss=0.87053  val_loss=0.86794\n",
      "Epoch 44: loss=0.86305  val_loss=0.86036\n",
      "Epoch 45: loss=0.85529  val_loss=0.85244\n",
      "Epoch 46: loss=0.84757  val_loss=0.84461\n",
      "Epoch 47: loss=0.83962  val_loss=0.83657\n",
      "Epoch 48: loss=0.83172  val_loss=0.82859\n",
      "Epoch 49: loss=0.82369  val_loss=0.82052\n",
      "Epoch 50: loss=0.81567  val_loss=0.81231\n",
      "Epoch 51: loss=0.80746  val_loss=0.80420\n",
      "Epoch 52: loss=0.79913  val_loss=0.79567\n",
      "Epoch 53: loss=0.79072  val_loss=0.78744\n",
      "Epoch 54: loss=0.78226  val_loss=0.77889\n",
      "Epoch 55: loss=0.77387  val_loss=0.77063\n",
      "Epoch 56: loss=0.76534  val_loss=0.76202\n",
      "Epoch 57: loss=0.75687  val_loss=0.75370\n",
      "Epoch 58: loss=0.74818  val_loss=0.74504\n",
      "Epoch 59: loss=0.73966  val_loss=0.73662\n",
      "Epoch 60: loss=0.73088  val_loss=0.72765\n",
      "Epoch 61: loss=0.72232  val_loss=0.71910\n",
      "Epoch 62: loss=0.71346  val_loss=0.71017\n",
      "Epoch 63: loss=0.70495  val_loss=0.70169\n",
      "Epoch 64: loss=0.69623  val_loss=0.69291\n",
      "Epoch 65: loss=0.68768  val_loss=0.68441\n",
      "Epoch 66: loss=0.67901  val_loss=0.67580\n",
      "Epoch 67: loss=0.67041  val_loss=0.66730\n",
      "Epoch 68: loss=0.66178  val_loss=0.65874\n",
      "Epoch 69: loss=0.65328  val_loss=0.65034\n",
      "Epoch 70: loss=0.64495  val_loss=0.64202\n",
      "Epoch 71: loss=0.63677  val_loss=0.63387\n",
      "Epoch 72: loss=0.62863  val_loss=0.62559\n",
      "Epoch 73: loss=0.62066  val_loss=0.61757\n",
      "Epoch 74: loss=0.61280  val_loss=0.60968\n",
      "Epoch 75: loss=0.60501  val_loss=0.60193\n",
      "Epoch 76: loss=0.59731  val_loss=0.59424\n",
      "Epoch 77: loss=0.58966  val_loss=0.58663\n",
      "Epoch 78: loss=0.58213  val_loss=0.57911\n",
      "Epoch 79: loss=0.57469  val_loss=0.57167\n",
      "Epoch 80: loss=0.56737  val_loss=0.56429\n",
      "Epoch 81: loss=0.56019  val_loss=0.55712\n",
      "Epoch 82: loss=0.55312  val_loss=0.54999\n",
      "Epoch 83: loss=0.54615  val_loss=0.54303\n",
      "Epoch 84: loss=0.53931  val_loss=0.53615\n",
      "Epoch 85: loss=0.53258  val_loss=0.52939\n",
      "Epoch 86: loss=0.52597  val_loss=0.52276\n",
      "Epoch 87: loss=0.51945  val_loss=0.51624\n",
      "Epoch 88: loss=0.51304  val_loss=0.50984\n",
      "Epoch 89: loss=0.50671  val_loss=0.50351\n",
      "Epoch 90: loss=0.50050  val_loss=0.49736\n",
      "Epoch 91: loss=0.49440  val_loss=0.49132\n",
      "Epoch 92: loss=0.48837  val_loss=0.48530\n",
      "Epoch 93: loss=0.48245  val_loss=0.47940\n",
      "Epoch 94: loss=0.47663  val_loss=0.47361\n",
      "Epoch 95: loss=0.47090  val_loss=0.46790\n",
      "Epoch 96: loss=0.46525  val_loss=0.46229\n",
      "Epoch 97: loss=0.45971  val_loss=0.45679\n",
      "Epoch 98: loss=0.45427  val_loss=0.45133\n",
      "Epoch 99: loss=0.44893  val_loss=0.44601\n",
      "Epoch 100: loss=0.44372  val_loss=0.44078\n",
      "Epoch 101: loss=0.43859  val_loss=0.43567\n",
      "Epoch 102: loss=0.43355  val_loss=0.43062\n",
      "Epoch 103: loss=0.42861  val_loss=0.42567\n",
      "Epoch 104: loss=0.42375  val_loss=0.42082\n",
      "Epoch 105: loss=0.41896  val_loss=0.41605\n",
      "Epoch 106: loss=0.41426  val_loss=0.41135\n",
      "Epoch 107: loss=0.40963  val_loss=0.40673\n",
      "Epoch 108: loss=0.40509  val_loss=0.40217\n",
      "Epoch 109: loss=0.40065  val_loss=0.39775\n",
      "Epoch 110: loss=0.39628  val_loss=0.39337\n",
      "Epoch 111: loss=0.39199  val_loss=0.38906\n",
      "Epoch 112: loss=0.38778  val_loss=0.38479\n",
      "Epoch 113: loss=0.38363  val_loss=0.38060\n",
      "Epoch 114: loss=0.37955  val_loss=0.37649\n",
      "Epoch 115: loss=0.37555  val_loss=0.37242\n",
      "Epoch 116: loss=0.37162  val_loss=0.36846\n",
      "Epoch 117: loss=0.36776  val_loss=0.36454\n",
      "Epoch 118: loss=0.36397  val_loss=0.36070\n",
      "Epoch 119: loss=0.36024  val_loss=0.35688\n",
      "Epoch 120: loss=0.35657  val_loss=0.35317\n",
      "Epoch 121: loss=0.35295  val_loss=0.34949\n",
      "Epoch 122: loss=0.34939  val_loss=0.34587\n",
      "Epoch 123: loss=0.34589  val_loss=0.34232\n",
      "Epoch 124: loss=0.34243  val_loss=0.33878\n",
      "Epoch 125: loss=0.33902  val_loss=0.33531\n",
      "Epoch 126: loss=0.33565  val_loss=0.33190\n",
      "Epoch 127: loss=0.33233  val_loss=0.32849\n",
      "Epoch 128: loss=0.32905  val_loss=0.32513\n",
      "Epoch 129: loss=0.32583  val_loss=0.32178\n",
      "Epoch 130: loss=0.32264  val_loss=0.31852\n",
      "Epoch 131: loss=0.31950  val_loss=0.31522\n",
      "Epoch 132: loss=0.31641  val_loss=0.31204\n",
      "Epoch 133: loss=0.31337  val_loss=0.30886\n",
      "Epoch 134: loss=0.31036  val_loss=0.30574\n",
      "Epoch 135: loss=0.30740  val_loss=0.30260\n",
      "Epoch 136: loss=0.30447  val_loss=0.29958\n",
      "Epoch 137: loss=0.30158  val_loss=0.29655\n",
      "Epoch 138: loss=0.29873  val_loss=0.29358\n",
      "Epoch 139: loss=0.29592  val_loss=0.29061\n",
      "Epoch 140: loss=0.29316  val_loss=0.28776\n",
      "Epoch 141: loss=0.29043  val_loss=0.28488\n",
      "Epoch 142: loss=0.28773  val_loss=0.28203\n",
      "Epoch 143: loss=0.28508  val_loss=0.27932\n",
      "Epoch 144: loss=0.28245  val_loss=0.27646\n",
      "Epoch 145: loss=0.27986  val_loss=0.27382\n",
      "Epoch 146: loss=0.27729  val_loss=0.27100\n",
      "Epoch 147: loss=0.27476  val_loss=0.26844\n",
      "Epoch 148: loss=0.27226  val_loss=0.26569\n",
      "Epoch 149: loss=0.26979  val_loss=0.26315\n",
      "Epoch 150: loss=0.26735  val_loss=0.26056\n",
      "Epoch 151: loss=0.26494  val_loss=0.25803\n",
      "Epoch 152: loss=0.26256  val_loss=0.25548\n",
      "Epoch 153: loss=0.26021  val_loss=0.25297\n",
      "Epoch 154: loss=0.25789  val_loss=0.25051\n",
      "Epoch 155: loss=0.25559  val_loss=0.24794\n",
      "Epoch 156: loss=0.25333  val_loss=0.24559\n",
      "Epoch 157: loss=0.25109  val_loss=0.24311\n",
      "Epoch 158: loss=0.24888  val_loss=0.24080\n",
      "Epoch 159: loss=0.24670  val_loss=0.23844\n",
      "Epoch 160: loss=0.24455  val_loss=0.23615\n",
      "Epoch 161: loss=0.24242  val_loss=0.23385\n",
      "Epoch 162: loss=0.24032  val_loss=0.23160\n",
      "Epoch 163: loss=0.23824  val_loss=0.22937\n",
      "Epoch 164: loss=0.23618  val_loss=0.22713\n",
      "Epoch 165: loss=0.23415  val_loss=0.22495\n",
      "Epoch 166: loss=0.23214  val_loss=0.22282\n",
      "Epoch 167: loss=0.23016  val_loss=0.22065\n",
      "Epoch 168: loss=0.22820  val_loss=0.21860\n",
      "Epoch 169: loss=0.22626  val_loss=0.21649\n",
      "Epoch 170: loss=0.22434  val_loss=0.21451\n",
      "Epoch 171: loss=0.22245  val_loss=0.21240\n",
      "Epoch 172: loss=0.22059  val_loss=0.21055\n",
      "Epoch 173: loss=0.21874  val_loss=0.20839\n",
      "Epoch 174: loss=0.21691  val_loss=0.20668\n",
      "Epoch 175: loss=0.21511  val_loss=0.20454\n",
      "Epoch 176: loss=0.21332  val_loss=0.20287\n",
      "Epoch 177: loss=0.21156  val_loss=0.20080\n",
      "Epoch 178: loss=0.20981  val_loss=0.19913\n",
      "Epoch 179: loss=0.20809  val_loss=0.19710\n",
      "Epoch 180: loss=0.20638  val_loss=0.19545\n",
      "Epoch 181: loss=0.20470  val_loss=0.19345\n",
      "Epoch 182: loss=0.20304  val_loss=0.19192\n",
      "Epoch 183: loss=0.20140  val_loss=0.18989\n",
      "Epoch 184: loss=0.19977  val_loss=0.18849\n",
      "Epoch 185: loss=0.19817  val_loss=0.18636\n",
      "Epoch 186: loss=0.19659  val_loss=0.18520\n",
      "Epoch 187: loss=0.19505  val_loss=0.18280\n",
      "Epoch 188: loss=0.19351  val_loss=0.18202\n",
      "Epoch 189: loss=0.19201  val_loss=0.17936\n",
      "Epoch 190: loss=0.19051  val_loss=0.17893\n",
      "Epoch 191: loss=0.18903  val_loss=0.17596\n",
      "Epoch 192: loss=0.18757  val_loss=0.17594\n",
      "Epoch 193: loss=0.18614  val_loss=0.17269\n",
      "Epoch 194: loss=0.18468  val_loss=0.17297\n",
      "Epoch 195: loss=0.18326  val_loss=0.16946\n",
      "Epoch 196: loss=0.18185  val_loss=0.17005\n",
      "Epoch 197: loss=0.18047  val_loss=0.16632\n",
      "Epoch 198: loss=0.17909  val_loss=0.16727\n",
      "Epoch 199: loss=0.17780  val_loss=0.16326\n",
      "Epoch 200: loss=0.17654  val_loss=0.16504\n",
      "Epoch 201: loss=0.17533  val_loss=0.16032\n",
      "Epoch 202: loss=0.17418  val_loss=0.16315\n",
      "Epoch 203: loss=0.17319  val_loss=0.15745\n",
      "Epoch 204: loss=0.17231  val_loss=0.16212\n",
      "Epoch 205: loss=0.17172  val_loss=0.15488\n",
      "Epoch 206: loss=0.17129  val_loss=0.16243\n",
      "Epoch 207: loss=0.17118  val_loss=0.15300\n",
      "Epoch 208: loss=0.17180  val_loss=0.16479\n",
      "Epoch 209: loss=0.17339  val_loss=0.15307\n",
      "Epoch 210: loss=0.17679  val_loss=0.17292\n",
      "Epoch 211: loss=0.18114  val_loss=0.15761\n",
      "Epoch 212: loss=0.19072  val_loss=0.19147\n",
      "Epoch 213: loss=0.20047  val_loss=0.17216\n",
      "Epoch 214: loss=0.22371  val_loss=0.23089\n",
      "Epoch 215: loss=0.23960  val_loss=0.20477\n",
      "Epoch 216: loss=0.29518  val_loss=0.31022\n",
      "Epoch 217: loss=0.31300  val_loss=0.26932\n",
      "Epoch 218: loss=0.41166  val_loss=0.43196\n",
      "Epoch 219: loss=0.33973  val_loss=0.29321\n",
      "Epoch 220: loss=0.42560  val_loss=0.44541\n",
      "Epoch 221: loss=0.30973  val_loss=0.26625\n",
      "Epoch 222: loss=0.35712  val_loss=0.37781\n",
      "Epoch 223: loss=0.27347  val_loss=0.23457\n",
      "Epoch 224: loss=0.29483  val_loss=0.31058\n",
      "Epoch 225: loss=0.24374  val_loss=0.20864\n",
      "Epoch 226: loss=0.24991  val_loss=0.26115\n",
      "Epoch 227: loss=0.21454  val_loss=0.18350\n",
      "Epoch 228: loss=0.21425  val_loss=0.22073\n",
      "Epoch 229: loss=0.19341  val_loss=0.16576\n",
      "Epoch 230: loss=0.19189  val_loss=0.19478\n",
      "Epoch 231: loss=0.18026  val_loss=0.15473\n",
      "Epoch 232: loss=0.17861  val_loss=0.17945\n",
      "Epoch 233: loss=0.17082  val_loss=0.14694\n",
      "Epoch 234: loss=0.16853  val_loss=0.16779\n",
      "Epoch 235: loss=0.16321  val_loss=0.14072\n",
      "Epoch 236: loss=0.16123  val_loss=0.15919\n",
      "Epoch 237: loss=0.15732  val_loss=0.13592\n",
      "Epoch 238: loss=0.15537  val_loss=0.15207\n",
      "Epoch 239: loss=0.15281  val_loss=0.13222\n",
      "Epoch 240: loss=0.15093  val_loss=0.14681\n",
      "Epoch 241: loss=0.14874  val_loss=0.12894\n",
      "Epoch 242: loss=0.14715  val_loss=0.14226\n",
      "Epoch 243: loss=0.14588  val_loss=0.12652\n",
      "Epoch 244: loss=0.14543  val_loss=0.14124\n",
      "Epoch 245: loss=0.14446  val_loss=0.12520\n",
      "Epoch 246: loss=0.14474  val_loss=0.14163\n",
      "Epoch 247: loss=0.14398  val_loss=0.12441\n",
      "Epoch 248: loss=0.14474  val_loss=0.14291\n",
      "Epoch 249: loss=0.14433  val_loss=0.12416\n",
      "Epoch 250: loss=0.14597  val_loss=0.14576\n",
      "Epoch 251: loss=0.14607  val_loss=0.12484\n",
      "Epoch 252: loss=0.14896  val_loss=0.15075\n",
      "Epoch 253: loss=0.14946  val_loss=0.12674\n",
      "Epoch 254: loss=0.15383  val_loss=0.15764\n",
      "Epoch 255: loss=0.15460  val_loss=0.13013\n",
      "Epoch 256: loss=0.16157  val_loss=0.16757\n",
      "Epoch 257: loss=0.16295  val_loss=0.13624\n",
      "Epoch 258: loss=0.17357  val_loss=0.18229\n",
      "Epoch 259: loss=0.17371  val_loss=0.14444\n",
      "Epoch 260: loss=0.18965  val_loss=0.20126\n",
      "Epoch 261: loss=0.19097  val_loss=0.15816\n",
      "Epoch 262: loss=0.21340  val_loss=0.22769\n",
      "Epoch 263: loss=0.21535  val_loss=0.17813\n",
      "Epoch 264: loss=0.25259  val_loss=0.27056\n",
      "Epoch 265: loss=0.24835  val_loss=0.20592\n",
      "Epoch 266: loss=0.30035  val_loss=0.32177\n",
      "Epoch 267: loss=0.27213  val_loss=0.22616\n",
      "Epoch 268: loss=0.32463  val_loss=0.34836\n",
      "Epoch 269: loss=0.26098  val_loss=0.21672\n",
      "Epoch 270: loss=0.29203  val_loss=0.31285\n",
      "Epoch 271: loss=0.23806  val_loss=0.19735\n",
      "Epoch 272: loss=0.25090  val_loss=0.26781\n",
      "Epoch 273: loss=0.20461  val_loss=0.16980\n",
      "Epoch 274: loss=0.20457  val_loss=0.21696\n",
      "Epoch 275: loss=0.17179  val_loss=0.14313\n",
      "Epoch 276: loss=0.16397  val_loss=0.17142\n",
      "Epoch 277: loss=0.14739  val_loss=0.12390\n",
      "Epoch 278: loss=0.14104  val_loss=0.14520\n",
      "Epoch 279: loss=0.13326  val_loss=0.11362\n",
      "Epoch 280: loss=0.12929  val_loss=0.13179\n",
      "Epoch 281: loss=0.12425  val_loss=0.10804\n",
      "Epoch 282: loss=0.12170  val_loss=0.12273\n",
      "Epoch 283: loss=0.11908  val_loss=0.10534\n",
      "Epoch 284: loss=0.11723  val_loss=0.11763\n",
      "Epoch 285: loss=0.11542  val_loss=0.10385\n",
      "Epoch 286: loss=0.11409  val_loss=0.11398\n",
      "Epoch 287: loss=0.11261  val_loss=0.10280\n",
      "Epoch 288: loss=0.11150  val_loss=0.11088\n",
      "Epoch 289: loss=0.11035  val_loss=0.10201\n",
      "Epoch 290: loss=0.10934  val_loss=0.10828\n",
      "Epoch 291: loss=0.10839  val_loss=0.10125\n",
      "Epoch 292: loss=0.10752  val_loss=0.10633\n",
      "Epoch 293: loss=0.10671  val_loss=0.10056\n",
      "Epoch 294: loss=0.10595  val_loss=0.10495\n",
      "Epoch 295: loss=0.10521  val_loss=0.09989\n",
      "Epoch 296: loss=0.10448  val_loss=0.10374\n",
      "Epoch 297: loss=0.10378  val_loss=0.09926\n",
      "Epoch 298: loss=0.10309  val_loss=0.10265\n",
      "Epoch 299: loss=0.10241  val_loss=0.09870\n",
      "[[10  0  0]\n",
      " [ 0  9  1]\n",
      " [ 0  0 10]]\n"
     ]
    }
   ],
   "source": [
    "# Define a training loop\n",
    "epochs = range(300)    \n",
    "# print(y_train[:5],dense_model(X_train[:5]))\n",
    "# print(loss(y_train[:5],dense_model(X_train[:5])))\n",
    "# print(loss(y_train[:5], dense_model(X_train[:5])))\n",
    "# Do the training\n",
    "training_loop(dense_model, X_train, y_train,validset=[X_test,y_test])\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "y_pred_test = dense_model(X_test).numpy()\n",
    "y_pred_test = np.argmax(y_pred_test, axis=1).tolist()\n",
    "# y_test = np.argmax(y_test, axis=1).tolist()\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "# print(metrics.classification_report(y_test, y_pred_test, digits=3))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce43f5afc3da1ea9c2859aca36b65d9af6136ef930ed7edf27ba0e49c79ddf9d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('pro1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
