{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Blueprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import copy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import requests\n",
    "import pandas_datareader.data as web\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import Series\n",
    "from numpy import nan\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "# from flask import jsonify/\n",
    "\n",
    "path = os.getcwd()\n",
    "csv_list = sorted(glob(os.path.join(path,\"DB\",\"*.csv\")))\n",
    "\n",
    "data = pd.read_csv(csv_list[1])\n",
    "data.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Blueprint\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "def train(model, x, y):\n",
    "  with tf.GradientTape() as t:\n",
    "    # Trainable variables are automatically tracked by GradientTape\n",
    "    current_loss = loss(y_true=y,y_pred=model(x, training=True))\n",
    "\n",
    "  # Use GradientTape to calculate the gradients with respect to W and b\n",
    "  grads= t.gradient(current_loss, model.trainable_variables)\n",
    "  \n",
    "  # Subtract the gradient scaled by the learning rate\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "def training_loop(model, x, y, validset = None):\n",
    "    # Keep results for plotting\n",
    "    train_loss_results = []\n",
    "    val_loss_results = []\n",
    "    for epoch in epochs:\n",
    "        # Update the model with the single giant batch\n",
    "        train(model, x, y)\n",
    "\n",
    "        # Track this before I update\n",
    "        if validset == None:\n",
    "            current_loss = loss(y,model(x))\n",
    "            print(\"Epoch %2d: loss=%2.5f\" %\n",
    "                (epoch, current_loss))\n",
    "        else:\n",
    "            x_val = validset[0]\n",
    "            y_val = validset[1]\n",
    "\n",
    "            current_loss = loss(y_true=y,y_pred=model(x))\n",
    "            valid_loss = loss(y_true=y_val,y_pred=model(x_val))\n",
    "            print(\"Epoch %2d: loss=%2.5f  val_loss=%2.5f\" %\n",
    "                (epoch, current_loss, valid_loss))\n",
    "            # save loss history\n",
    "            train_loss_results.append(current_loss.numpy())\n",
    "            val_loss_results.append(valid_loss.numpy())\n",
    "    return train_loss_results,val_loss_results\n",
    "            \n",
    "\n",
    "\n",
    "path = os.getcwd()\n",
    "csv_list = sorted(glob(os.path.join(path,\"DB\",\"*.csv\")))\n",
    "data = pd.read_csv(csv_list[1])\n",
    "trainset, testset = train_test_split(data,test_size=0.2,shuffle=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = trainset.values[:,:-1]\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "y_train = trainset.values[:,-1]\n",
    "# y_train = pd.get_dummies(trainset.values[:,-1]).values\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "\n",
    "X_test = testset.values[:,:-1]\n",
    "X_test = scaler.transform(X_test)\n",
    "y_test = testset.values[:,-1]\n",
    "# y_test = pd.get_dummies(testset.values[:,-1]).values\n",
    "y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "\n",
    "\n",
    "#params\n",
    "input_shape = 4\n",
    "output_shape = 3\n",
    "lr = 0.03\n",
    "act_function = 'relu'\n",
    "outlayer_act_function = 'softmax'\n",
    "category = \"classification\"\n",
    "#optimizer\n",
    "optimizer = keras.optimizers.SGD(learning_rate=lr)\n",
    "\n",
    "# #loss function\n",
    "loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "num_perceptron_layer1 = 8\n",
    "num_perceptron_layer2 = 16\n",
    "num_perceptron_layer3 = 32\n",
    "num_perceptron_layer4 = 16\n",
    "num_perceptron_layer5 = 8\n",
    "num_perceptron_out = output_shape\n",
    "\n",
    "num_perceptron_list = [\n",
    "    num_perceptron_layer1,\n",
    "    num_perceptron_layer2,\n",
    "    num_perceptron_layer3,\n",
    "    num_perceptron_layer4,\n",
    "    num_perceptron_layer5,\n",
    "    num_perceptron_out,\n",
    "    ]\n",
    "\n",
    "# model definition\n",
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "for i in range(len(num_perceptron_list)):\n",
    "    n_percep = num_perceptron_list[i]\n",
    "    if i == 0:\n",
    "        x = keras.layers.Dense(n_percep, activation= act_function)(inputs)\n",
    "\n",
    "    elif  i != 0 and n_percep != 0 and i != len(num_perceptron_list)-1:\n",
    "        x = keras.layers.Dense(n_percep, activation= act_function)(x)\n",
    "\n",
    "    elif  i == len(num_perceptron_list)-1:\n",
    "        x = keras.layers.Dense(n_percep,)(x)#activation= outlayer_act_function)(x)\n",
    "\n",
    "    else: pass\n",
    "\n",
    "dense_model = tf.keras.Model(name=\"model_1\",inputs=inputs, outputs=x)\n",
    "# dense_model.summary()\n",
    "\n",
    "\n",
    "# Define a training loop\n",
    "epochs = range(300)    \n",
    "# print(y_train[:5],dense_model(X_train[:5]))\n",
    "# print(loss(y_train[:5],dense_model(X_train[:5])))\n",
    "# print(loss(y_train[:5], dense_model(X_train[:5])))\n",
    "# Do the training\n",
    "# training_loop(dense_model, X_train, y_train,validset=[X_test,y_test])\n",
    "\n",
    "\n",
    "# from sklearn import metrics\n",
    "# y_pred_test = dense_model(X_test).numpy()\n",
    "# y_pred_test = np.argmax(y_pred_test, axis=1).tolist()\n",
    "# # y_test = np.argmax(y_test, axis=1).tolist()\n",
    "\n",
    "# # Print the confusion matrix\n",
    "# print(metrics.confusion_matrix(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0: loss=1.07314  val_loss=1.09133\n",
      "Epoch  1: loss=1.07192  val_loss=1.09090\n",
      "Epoch  2: loss=1.07084  val_loss=1.09046\n",
      "Epoch  3: loss=1.06986  val_loss=1.08993\n",
      "Epoch  4: loss=1.06888  val_loss=1.08938\n",
      "Epoch  5: loss=1.06790  val_loss=1.08879\n",
      "Epoch  6: loss=1.06691  val_loss=1.08818\n",
      "Epoch  7: loss=1.06593  val_loss=1.08757\n",
      "Epoch  8: loss=1.06494  val_loss=1.08696\n",
      "Epoch  9: loss=1.06394  val_loss=1.08633\n",
      "Epoch 10: loss=1.06291  val_loss=1.08569\n",
      "Epoch 11: loss=1.06187  val_loss=1.08502\n",
      "Epoch 12: loss=1.06081  val_loss=1.08433\n",
      "Epoch 13: loss=1.05973  val_loss=1.08363\n",
      "Epoch 14: loss=1.05862  val_loss=1.08292\n",
      "Epoch 15: loss=1.05749  val_loss=1.08219\n",
      "Epoch 16: loss=1.05635  val_loss=1.08144\n",
      "Epoch 17: loss=1.05518  val_loss=1.08065\n",
      "Epoch 18: loss=1.05400  val_loss=1.07983\n",
      "Epoch 19: loss=1.05281  val_loss=1.07899\n",
      "Epoch 20: loss=1.05159  val_loss=1.07812\n",
      "Epoch 21: loss=1.05038  val_loss=1.07725\n",
      "Epoch 22: loss=1.04918  val_loss=1.07645\n",
      "Epoch 23: loss=1.04797  val_loss=1.07566\n",
      "Epoch 24: loss=1.04674  val_loss=1.07485\n",
      "Epoch 25: loss=1.04551  val_loss=1.07406\n",
      "Epoch 26: loss=1.04425  val_loss=1.07326\n",
      "Epoch 27: loss=1.04298  val_loss=1.07245\n",
      "Epoch 28: loss=1.04170  val_loss=1.07163\n",
      "Epoch 29: loss=1.04040  val_loss=1.07081\n",
      "Epoch 30: loss=1.03906  val_loss=1.06997\n",
      "Epoch 31: loss=1.03770  val_loss=1.06915\n",
      "Epoch 32: loss=1.03630  val_loss=1.06830\n",
      "Epoch 33: loss=1.03490  val_loss=1.06746\n",
      "Epoch 34: loss=1.03347  val_loss=1.06662\n",
      "Epoch 35: loss=1.03202  val_loss=1.06578\n",
      "Epoch 36: loss=1.03054  val_loss=1.06488\n",
      "Epoch 37: loss=1.02901  val_loss=1.06390\n",
      "Epoch 38: loss=1.02745  val_loss=1.06287\n",
      "Epoch 39: loss=1.02585  val_loss=1.06177\n",
      "Epoch 40: loss=1.02424  val_loss=1.06071\n",
      "Epoch 41: loss=1.02261  val_loss=1.05960\n",
      "Epoch 42: loss=1.02095  val_loss=1.05844\n",
      "Epoch 43: loss=1.01928  val_loss=1.05727\n",
      "Epoch 44: loss=1.01752  val_loss=1.05601\n",
      "Epoch 45: loss=1.01568  val_loss=1.05471\n",
      "Epoch 46: loss=1.01357  val_loss=1.05321\n",
      "Epoch 47: loss=1.01109  val_loss=1.05127\n",
      "Epoch 48: loss=1.00826  val_loss=1.04897\n",
      "Epoch 49: loss=1.00547  val_loss=1.04662\n",
      "Epoch 50: loss=1.00282  val_loss=1.04441\n",
      "Epoch 51: loss=1.00025  val_loss=1.04244\n",
      "Epoch 52: loss=0.99780  val_loss=1.04061\n",
      "Epoch 53: loss=0.99547  val_loss=1.03889\n",
      "Epoch 54: loss=0.99319  val_loss=1.03730\n",
      "Epoch 55: loss=0.99093  val_loss=1.03576\n",
      "Epoch 56: loss=0.98867  val_loss=1.03422\n",
      "Epoch 57: loss=0.98640  val_loss=1.03264\n",
      "Epoch 58: loss=0.98413  val_loss=1.03105\n",
      "Epoch 59: loss=0.98184  val_loss=1.02947\n",
      "Epoch 60: loss=0.97954  val_loss=1.02786\n",
      "Epoch 61: loss=0.97723  val_loss=1.02626\n",
      "Epoch 62: loss=0.97490  val_loss=1.02463\n",
      "Epoch 63: loss=0.97256  val_loss=1.02298\n",
      "Epoch 64: loss=0.97020  val_loss=1.02133\n",
      "Epoch 65: loss=0.96782  val_loss=1.01967\n",
      "Epoch 66: loss=0.96543  val_loss=1.01796\n",
      "Epoch 67: loss=0.96303  val_loss=1.01621\n",
      "Epoch 68: loss=0.96061  val_loss=1.01443\n",
      "Epoch 69: loss=0.95817  val_loss=1.01264\n",
      "Epoch 70: loss=0.95573  val_loss=1.01083\n",
      "Epoch 71: loss=0.95326  val_loss=1.00896\n",
      "Epoch 72: loss=0.95078  val_loss=1.00713\n",
      "Epoch 73: loss=0.94827  val_loss=1.00524\n",
      "Epoch 74: loss=0.94568  val_loss=1.00323\n",
      "Epoch 75: loss=0.94282  val_loss=1.00097\n",
      "Epoch 76: loss=0.93984  val_loss=0.99835\n",
      "Epoch 77: loss=0.93723  val_loss=0.99621\n",
      "Epoch 78: loss=0.93467  val_loss=0.99411\n",
      "Epoch 79: loss=0.93211  val_loss=0.99200\n",
      "Epoch 80: loss=0.92955  val_loss=0.98990\n",
      "Epoch 81: loss=0.92696  val_loss=0.98774\n",
      "Epoch 82: loss=0.92436  val_loss=0.98554\n",
      "Epoch 83: loss=0.92174  val_loss=0.98336\n",
      "Epoch 84: loss=0.91912  val_loss=0.98114\n",
      "Epoch 85: loss=0.91651  val_loss=0.97903\n",
      "Epoch 86: loss=0.91388  val_loss=0.97685\n",
      "Epoch 87: loss=0.91125  val_loss=0.97465\n",
      "Epoch 88: loss=0.90860  val_loss=0.97235\n",
      "Epoch 89: loss=0.90594  val_loss=0.97005\n",
      "Epoch 90: loss=0.90328  val_loss=0.96775\n",
      "Epoch 91: loss=0.90060  val_loss=0.96536\n",
      "Epoch 92: loss=0.89792  val_loss=0.96306\n",
      "Epoch 93: loss=0.89522  val_loss=0.96067\n",
      "Epoch 94: loss=0.89250  val_loss=0.95823\n",
      "Epoch 95: loss=0.88977  val_loss=0.95581\n",
      "Epoch 96: loss=0.88702  val_loss=0.95330\n",
      "Epoch 97: loss=0.88427  val_loss=0.95077\n",
      "Epoch 98: loss=0.88151  val_loss=0.94818\n",
      "Epoch 99: loss=0.87873  val_loss=0.94559\n",
      "Epoch 100: loss=0.87594  val_loss=0.94295\n",
      "Epoch 101: loss=0.87313  val_loss=0.94027\n",
      "Epoch 102: loss=0.87031  val_loss=0.93759\n",
      "Epoch 103: loss=0.86749  val_loss=0.93491\n",
      "Epoch 104: loss=0.86465  val_loss=0.93222\n",
      "Epoch 105: loss=0.86180  val_loss=0.92945\n",
      "Epoch 106: loss=0.85894  val_loss=0.92669\n",
      "Epoch 107: loss=0.85607  val_loss=0.92391\n",
      "Epoch 108: loss=0.85318  val_loss=0.92109\n",
      "Epoch 109: loss=0.85027  val_loss=0.91823\n",
      "Epoch 110: loss=0.84737  val_loss=0.91537\n",
      "Epoch 111: loss=0.84445  val_loss=0.91249\n",
      "Epoch 112: loss=0.84152  val_loss=0.90959\n",
      "Epoch 113: loss=0.83856  val_loss=0.90669\n",
      "Epoch 114: loss=0.83557  val_loss=0.90374\n",
      "Epoch 115: loss=0.83255  val_loss=0.90079\n",
      "Epoch 116: loss=0.82951  val_loss=0.89777\n",
      "Epoch 117: loss=0.82647  val_loss=0.89477\n",
      "Epoch 118: loss=0.82343  val_loss=0.89177\n",
      "Epoch 119: loss=0.82037  val_loss=0.88871\n",
      "Epoch 120: loss=0.81731  val_loss=0.88566\n",
      "Epoch 121: loss=0.81424  val_loss=0.88255\n",
      "Epoch 122: loss=0.81117  val_loss=0.87941\n",
      "Epoch 123: loss=0.80808  val_loss=0.87620\n",
      "Epoch 124: loss=0.80496  val_loss=0.87294\n",
      "Epoch 125: loss=0.80184  val_loss=0.86969\n",
      "Epoch 126: loss=0.79872  val_loss=0.86642\n",
      "Epoch 127: loss=0.79559  val_loss=0.86313\n",
      "Epoch 128: loss=0.79247  val_loss=0.85983\n",
      "Epoch 129: loss=0.78932  val_loss=0.85652\n",
      "Epoch 130: loss=0.78617  val_loss=0.85316\n",
      "Epoch 131: loss=0.78302  val_loss=0.84978\n",
      "Epoch 132: loss=0.77986  val_loss=0.84640\n",
      "Epoch 133: loss=0.77670  val_loss=0.84301\n",
      "Epoch 134: loss=0.77354  val_loss=0.83962\n",
      "Epoch 135: loss=0.77039  val_loss=0.83624\n",
      "Epoch 136: loss=0.76726  val_loss=0.83287\n",
      "Epoch 137: loss=0.76411  val_loss=0.82949\n",
      "Epoch 138: loss=0.76097  val_loss=0.82610\n",
      "Epoch 139: loss=0.75784  val_loss=0.82271\n",
      "Epoch 140: loss=0.75471  val_loss=0.81933\n",
      "Epoch 141: loss=0.75157  val_loss=0.81591\n",
      "Epoch 142: loss=0.74841  val_loss=0.81248\n",
      "Epoch 143: loss=0.74527  val_loss=0.80908\n",
      "Epoch 144: loss=0.74212  val_loss=0.80570\n",
      "Epoch 145: loss=0.73898  val_loss=0.80229\n",
      "Epoch 146: loss=0.73583  val_loss=0.79888\n",
      "Epoch 147: loss=0.73268  val_loss=0.79546\n",
      "Epoch 148: loss=0.72950  val_loss=0.79200\n",
      "Epoch 149: loss=0.72633  val_loss=0.78858\n",
      "Epoch 150: loss=0.72313  val_loss=0.78512\n",
      "Epoch 151: loss=0.71993  val_loss=0.78165\n",
      "Epoch 152: loss=0.71674  val_loss=0.77819\n",
      "Epoch 153: loss=0.71355  val_loss=0.77472\n",
      "Epoch 154: loss=0.71036  val_loss=0.77126\n",
      "Epoch 155: loss=0.70718  val_loss=0.76783\n",
      "Epoch 156: loss=0.70402  val_loss=0.76441\n",
      "Epoch 157: loss=0.70086  val_loss=0.76100\n",
      "Epoch 158: loss=0.69771  val_loss=0.75759\n",
      "Epoch 159: loss=0.69456  val_loss=0.75417\n",
      "Epoch 160: loss=0.69142  val_loss=0.75073\n",
      "Epoch 161: loss=0.68829  val_loss=0.74730\n",
      "Epoch 162: loss=0.68514  val_loss=0.74385\n",
      "Epoch 163: loss=0.68200  val_loss=0.74041\n",
      "Epoch 164: loss=0.67885  val_loss=0.73696\n",
      "Epoch 165: loss=0.67570  val_loss=0.73349\n",
      "Epoch 166: loss=0.67257  val_loss=0.73003\n",
      "Epoch 167: loss=0.66944  val_loss=0.72657\n",
      "Epoch 168: loss=0.66632  val_loss=0.72312\n",
      "Epoch 169: loss=0.66321  val_loss=0.71968\n",
      "Epoch 170: loss=0.66011  val_loss=0.71627\n",
      "Epoch 171: loss=0.65700  val_loss=0.71285\n",
      "Epoch 172: loss=0.65391  val_loss=0.70941\n",
      "Epoch 173: loss=0.65084  val_loss=0.70603\n",
      "Epoch 174: loss=0.64779  val_loss=0.70266\n",
      "Epoch 175: loss=0.64474  val_loss=0.69930\n",
      "Epoch 176: loss=0.64170  val_loss=0.69593\n",
      "Epoch 177: loss=0.63868  val_loss=0.69263\n",
      "Epoch 178: loss=0.63568  val_loss=0.68927\n",
      "Epoch 179: loss=0.63269  val_loss=0.68597\n",
      "Epoch 180: loss=0.62971  val_loss=0.68262\n",
      "Epoch 181: loss=0.62674  val_loss=0.67925\n",
      "Epoch 182: loss=0.62378  val_loss=0.67591\n",
      "Epoch 183: loss=0.62083  val_loss=0.67257\n",
      "Epoch 184: loss=0.61788  val_loss=0.66921\n",
      "Epoch 185: loss=0.61495  val_loss=0.66587\n",
      "Epoch 186: loss=0.61201  val_loss=0.66249\n",
      "Epoch 187: loss=0.60908  val_loss=0.65914\n",
      "Epoch 188: loss=0.60616  val_loss=0.65577\n",
      "Epoch 189: loss=0.60324  val_loss=0.65244\n",
      "Epoch 190: loss=0.60032  val_loss=0.64912\n",
      "Epoch 191: loss=0.59740  val_loss=0.64577\n",
      "Epoch 192: loss=0.59449  val_loss=0.64243\n",
      "Epoch 193: loss=0.59158  val_loss=0.63910\n",
      "Epoch 194: loss=0.58867  val_loss=0.63579\n",
      "Epoch 195: loss=0.58579  val_loss=0.63251\n",
      "Epoch 196: loss=0.58291  val_loss=0.62925\n",
      "Epoch 197: loss=0.58004  val_loss=0.62602\n",
      "Epoch 198: loss=0.57719  val_loss=0.62278\n",
      "Epoch 199: loss=0.57435  val_loss=0.61957\n",
      "Epoch 200: loss=0.57153  val_loss=0.61639\n",
      "Epoch 201: loss=0.56873  val_loss=0.61321\n",
      "Epoch 202: loss=0.56595  val_loss=0.61005\n",
      "Epoch 203: loss=0.56319  val_loss=0.60691\n",
      "Epoch 204: loss=0.56045  val_loss=0.60378\n",
      "Epoch 205: loss=0.55772  val_loss=0.60067\n",
      "Epoch 206: loss=0.55502  val_loss=0.59759\n",
      "Epoch 207: loss=0.55233  val_loss=0.59453\n",
      "Epoch 208: loss=0.54967  val_loss=0.59148\n",
      "Epoch 209: loss=0.54702  val_loss=0.58846\n",
      "Epoch 210: loss=0.54439  val_loss=0.58545\n",
      "Epoch 211: loss=0.54178  val_loss=0.58246\n",
      "Epoch 212: loss=0.53920  val_loss=0.57952\n",
      "Epoch 213: loss=0.53664  val_loss=0.57660\n",
      "Epoch 214: loss=0.53409  val_loss=0.57370\n",
      "Epoch 215: loss=0.53157  val_loss=0.57082\n",
      "Epoch 216: loss=0.52907  val_loss=0.56797\n",
      "Epoch 217: loss=0.52659  val_loss=0.56514\n",
      "Epoch 218: loss=0.52413  val_loss=0.56234\n",
      "Epoch 219: loss=0.52169  val_loss=0.55954\n",
      "Epoch 220: loss=0.51928  val_loss=0.55679\n",
      "Epoch 221: loss=0.51688  val_loss=0.55406\n",
      "Epoch 222: loss=0.51451  val_loss=0.55135\n",
      "Epoch 223: loss=0.51216  val_loss=0.54865\n",
      "Epoch 224: loss=0.50983  val_loss=0.54598\n",
      "Epoch 225: loss=0.50752  val_loss=0.54335\n",
      "Epoch 226: loss=0.50524  val_loss=0.54074\n",
      "Epoch 227: loss=0.50297  val_loss=0.53816\n",
      "Epoch 228: loss=0.50072  val_loss=0.53561\n",
      "Epoch 229: loss=0.49850  val_loss=0.53308\n",
      "Epoch 230: loss=0.49629  val_loss=0.53057\n",
      "Epoch 231: loss=0.49411  val_loss=0.52809\n",
      "Epoch 232: loss=0.49195  val_loss=0.52562\n",
      "Epoch 233: loss=0.48980  val_loss=0.52318\n",
      "Epoch 234: loss=0.48768  val_loss=0.52076\n",
      "Epoch 235: loss=0.48558  val_loss=0.51836\n",
      "Epoch 236: loss=0.48350  val_loss=0.51599\n",
      "Epoch 237: loss=0.48144  val_loss=0.51365\n",
      "Epoch 238: loss=0.47940  val_loss=0.51135\n",
      "Epoch 239: loss=0.47738  val_loss=0.50907\n",
      "Epoch 240: loss=0.47538  val_loss=0.50680\n",
      "Epoch 241: loss=0.47340  val_loss=0.50457\n",
      "Epoch 242: loss=0.47144  val_loss=0.50235\n",
      "Epoch 243: loss=0.46949  val_loss=0.50017\n",
      "Epoch 244: loss=0.46758  val_loss=0.49802\n",
      "Epoch 245: loss=0.46568  val_loss=0.49589\n",
      "Epoch 246: loss=0.46380  val_loss=0.49377\n",
      "Epoch 247: loss=0.46194  val_loss=0.49167\n",
      "Epoch 248: loss=0.46010  val_loss=0.48960\n",
      "Epoch 249: loss=0.45828  val_loss=0.48757\n",
      "Epoch 250: loss=0.45648  val_loss=0.48557\n",
      "Epoch 251: loss=0.45470  val_loss=0.48358\n",
      "Epoch 252: loss=0.45293  val_loss=0.48160\n",
      "Epoch 253: loss=0.45119  val_loss=0.47965\n",
      "Epoch 254: loss=0.44946  val_loss=0.47772\n",
      "Epoch 255: loss=0.44775  val_loss=0.47583\n",
      "Epoch 256: loss=0.44606  val_loss=0.47393\n",
      "Epoch 257: loss=0.44438  val_loss=0.47206\n",
      "Epoch 258: loss=0.44272  val_loss=0.47020\n",
      "Epoch 259: loss=0.44108  val_loss=0.46838\n",
      "Epoch 260: loss=0.43945  val_loss=0.46656\n",
      "Epoch 261: loss=0.43784  val_loss=0.46478\n",
      "Epoch 262: loss=0.43624  val_loss=0.46302\n",
      "Epoch 263: loss=0.43466  val_loss=0.46129\n",
      "Epoch 264: loss=0.43310  val_loss=0.45957\n",
      "Epoch 265: loss=0.43155  val_loss=0.45787\n",
      "Epoch 266: loss=0.43002  val_loss=0.45620\n",
      "Epoch 267: loss=0.42850  val_loss=0.45452\n",
      "Epoch 268: loss=0.42700  val_loss=0.45288\n",
      "Epoch 269: loss=0.42551  val_loss=0.45125\n",
      "Epoch 270: loss=0.42403  val_loss=0.44963\n",
      "Epoch 271: loss=0.42257  val_loss=0.44802\n",
      "Epoch 272: loss=0.42112  val_loss=0.44643\n",
      "Epoch 273: loss=0.41969  val_loss=0.44485\n",
      "Epoch 274: loss=0.41827  val_loss=0.44329\n",
      "Epoch 275: loss=0.41686  val_loss=0.44174\n",
      "Epoch 276: loss=0.41546  val_loss=0.44022\n",
      "Epoch 277: loss=0.41408  val_loss=0.43871\n",
      "Epoch 278: loss=0.41270  val_loss=0.43720\n",
      "Epoch 279: loss=0.41134  val_loss=0.43571\n",
      "Epoch 280: loss=0.41000  val_loss=0.43423\n",
      "Epoch 281: loss=0.40866  val_loss=0.43278\n",
      "Epoch 282: loss=0.40734  val_loss=0.43136\n",
      "Epoch 283: loss=0.40603  val_loss=0.42996\n",
      "Epoch 284: loss=0.40473  val_loss=0.42857\n",
      "Epoch 285: loss=0.40344  val_loss=0.42719\n",
      "Epoch 286: loss=0.40216  val_loss=0.42582\n",
      "Epoch 287: loss=0.40089  val_loss=0.42445\n",
      "Epoch 288: loss=0.39964  val_loss=0.42312\n",
      "Epoch 289: loss=0.39839  val_loss=0.42182\n",
      "Epoch 290: loss=0.39716  val_loss=0.42052\n",
      "Epoch 291: loss=0.39594  val_loss=0.41923\n",
      "Epoch 292: loss=0.39472  val_loss=0.41795\n",
      "Epoch 293: loss=0.39352  val_loss=0.41667\n",
      "Epoch 294: loss=0.39233  val_loss=0.41539\n",
      "Epoch 295: loss=0.39114  val_loss=0.41414\n",
      "Epoch 296: loss=0.38997  val_loss=0.41288\n",
      "Epoch 297: loss=0.38880  val_loss=0.41164\n",
      "Epoch 298: loss=0.38764  val_loss=0.41040\n",
      "Epoch 299: loss=0.38649  val_loss=0.40918\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = training_loop(dense_model, X_train, y_train,validset=[X_test,y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0731417,\n",
       " 1.0719198,\n",
       " 1.0708374,\n",
       " 1.069859,\n",
       " 1.0688819,\n",
       " 1.0678983,\n",
       " 1.0669118,\n",
       " 1.0659312,\n",
       " 1.0649389,\n",
       " 1.0639391,\n",
       " 1.062912,\n",
       " 1.061874,\n",
       " 1.0608132,\n",
       " 1.0597281,\n",
       " 1.0586226,\n",
       " 1.0574933,\n",
       " 1.056346,\n",
       " 1.0551758,\n",
       " 1.054001,\n",
       " 1.0528057,\n",
       " 1.0515925,\n",
       " 1.0503774,\n",
       " 1.049179,\n",
       " 1.0479696,\n",
       " 1.0467415,\n",
       " 1.0455058,\n",
       " 1.0442542,\n",
       " 1.0429847,\n",
       " 1.0417013,\n",
       " 1.0403972,\n",
       " 1.0390639,\n",
       " 1.0376979,\n",
       " 1.0363002,\n",
       " 1.0348952,\n",
       " 1.0334691,\n",
       " 1.0320154,\n",
       " 1.0305359,\n",
       " 1.0290105,\n",
       " 1.0274485,\n",
       " 1.025848,\n",
       " 1.0242429,\n",
       " 1.0226077,\n",
       " 1.0209503,\n",
       " 1.0192816,\n",
       " 1.0175245,\n",
       " 1.0156823,\n",
       " 1.0135654,\n",
       " 1.0110853,\n",
       " 1.0082625,\n",
       " 1.0054735,\n",
       " 1.0028225,\n",
       " 1.0002545,\n",
       " 0.997798,\n",
       " 0.99546564,\n",
       " 0.9931856,\n",
       " 0.99093145,\n",
       " 0.9886743,\n",
       " 0.98640287,\n",
       " 0.98412627,\n",
       " 0.9818436,\n",
       " 0.9795442,\n",
       " 0.97723424,\n",
       " 0.97490275,\n",
       " 0.9725569,\n",
       " 0.97019756,\n",
       " 0.967823,\n",
       " 0.9654325,\n",
       " 0.9630274,\n",
       " 0.9606079,\n",
       " 0.9581722,\n",
       " 0.95572525,\n",
       " 0.95326,\n",
       " 0.95077515,\n",
       " 0.94826764,\n",
       " 0.94568425,\n",
       " 0.94281924,\n",
       " 0.9398394,\n",
       " 0.93723094,\n",
       " 0.9346712,\n",
       " 0.9321108,\n",
       " 0.929548,\n",
       " 0.92696005,\n",
       " 0.92436004,\n",
       " 0.92174137,\n",
       " 0.91912264,\n",
       " 0.91650784,\n",
       " 0.9138841,\n",
       " 0.9112494,\n",
       " 0.90860176,\n",
       " 0.9059441,\n",
       " 0.9032778,\n",
       " 0.9006011,\n",
       " 0.8979191,\n",
       " 0.8952187,\n",
       " 0.8924987,\n",
       " 0.88976616,\n",
       " 0.8870191,\n",
       " 0.8842662,\n",
       " 0.88150674,\n",
       " 0.87873405,\n",
       " 0.8759416,\n",
       " 0.8731319,\n",
       " 0.87031454,\n",
       " 0.86748695,\n",
       " 0.8646504,\n",
       " 0.8618019,\n",
       " 0.8589411,\n",
       " 0.85606796,\n",
       " 0.8531754,\n",
       " 0.8502734,\n",
       " 0.84736633,\n",
       " 0.8444546,\n",
       " 0.84152377,\n",
       " 0.8385601,\n",
       " 0.83557034,\n",
       " 0.8325507,\n",
       " 0.82951355,\n",
       " 0.8264715,\n",
       " 0.82342505,\n",
       " 0.82037085,\n",
       " 0.8173116,\n",
       " 0.8142446,\n",
       " 0.8111708,\n",
       " 0.80808,\n",
       " 0.80495965,\n",
       " 0.801844,\n",
       " 0.7987165,\n",
       " 0.7955945,\n",
       " 0.7924654,\n",
       " 0.7893231,\n",
       " 0.7861702,\n",
       " 0.78301567,\n",
       " 0.7798596,\n",
       " 0.77670425,\n",
       " 0.77354074,\n",
       " 0.7703907,\n",
       " 0.7672556,\n",
       " 0.7641118,\n",
       " 0.7609748,\n",
       " 0.7578449,\n",
       " 0.75470966,\n",
       " 0.7515654,\n",
       " 0.74841046,\n",
       " 0.7452661,\n",
       " 0.7421197,\n",
       " 0.73897684,\n",
       " 0.7358321,\n",
       " 0.7326786,\n",
       " 0.72950256,\n",
       " 0.7263317,\n",
       " 0.7231336,\n",
       " 0.7199318,\n",
       " 0.7167409,\n",
       " 0.7135492,\n",
       " 0.71035826,\n",
       " 0.7071845,\n",
       " 0.70401746,\n",
       " 0.7008587,\n",
       " 0.6977139,\n",
       " 0.6945648,\n",
       " 0.6914225,\n",
       " 0.68828624,\n",
       " 0.68514454,\n",
       " 0.6819982,\n",
       " 0.678854,\n",
       " 0.67569894,\n",
       " 0.6725671,\n",
       " 0.6694416,\n",
       " 0.6663171,\n",
       " 0.66321045,\n",
       " 0.6601135,\n",
       " 0.6570045,\n",
       " 0.65390736,\n",
       " 0.6508429,\n",
       " 0.6477875,\n",
       " 0.6447383,\n",
       " 0.6417001,\n",
       " 0.63868076,\n",
       " 0.6356798,\n",
       " 0.6326924,\n",
       " 0.62970823,\n",
       " 0.62673664,\n",
       " 0.62377626,\n",
       " 0.6208277,\n",
       " 0.6178829,\n",
       " 0.61494666,\n",
       " 0.6120115,\n",
       " 0.6090845,\n",
       " 0.6061581,\n",
       " 0.60323596,\n",
       " 0.6003197,\n",
       " 0.5974026,\n",
       " 0.5944868,\n",
       " 0.59157634,\n",
       " 0.58867466,\n",
       " 0.5857861,\n",
       " 0.58290637,\n",
       " 0.5800434,\n",
       " 0.5771894,\n",
       " 0.57434756,\n",
       " 0.57152814,\n",
       " 0.56872547,\n",
       " 0.5659457,\n",
       " 0.5631869,\n",
       " 0.5604458,\n",
       " 0.5577217,\n",
       " 0.55501837,\n",
       " 0.55233437,\n",
       " 0.5496677,\n",
       " 0.5470194,\n",
       " 0.54439104,\n",
       " 0.54178214,\n",
       " 0.5392011,\n",
       " 0.53663826,\n",
       " 0.5340948,\n",
       " 0.531573,\n",
       " 0.5290705,\n",
       " 0.52658993,\n",
       " 0.52413195,\n",
       " 0.5216947,\n",
       " 0.51927906,\n",
       " 0.51688373,\n",
       " 0.51451194,\n",
       " 0.5121618,\n",
       " 0.5098323,\n",
       " 0.5075236,\n",
       " 0.50523555,\n",
       " 0.5029691,\n",
       " 0.5007237,\n",
       " 0.49849856,\n",
       " 0.49629408,\n",
       " 0.49411023,\n",
       " 0.49194714,\n",
       " 0.48980454,\n",
       " 0.48768198,\n",
       " 0.48557958,\n",
       " 0.48349726,\n",
       " 0.4814363,\n",
       " 0.47939727,\n",
       " 0.47737852,\n",
       " 0.47537965,\n",
       " 0.47339758,\n",
       " 0.47143623,\n",
       " 0.4694947,\n",
       " 0.4675752,\n",
       " 0.46567613,\n",
       " 0.4637968,\n",
       " 0.4619374,\n",
       " 0.46009833,\n",
       " 0.45828027,\n",
       " 0.45648015,\n",
       " 0.45469794,\n",
       " 0.45293376,\n",
       " 0.45118707,\n",
       " 0.44945812,\n",
       " 0.44774768,\n",
       " 0.44605502,\n",
       " 0.4443789,\n",
       " 0.44271877,\n",
       " 0.44107506,\n",
       " 0.43944725,\n",
       " 0.4378357,\n",
       " 0.43624058,\n",
       " 0.43466416,\n",
       " 0.4331025,\n",
       " 0.43155384,\n",
       " 0.43002102,\n",
       " 0.42850277,\n",
       " 0.42699808,\n",
       " 0.4255094,\n",
       " 0.4240344,\n",
       " 0.42257273,\n",
       " 0.42112485,\n",
       " 0.41969007,\n",
       " 0.4182673,\n",
       " 0.41685733,\n",
       " 0.41546008,\n",
       " 0.4140757,\n",
       " 0.41270405,\n",
       " 0.4113442,\n",
       " 0.4099961,\n",
       " 0.4086609,\n",
       " 0.40733835,\n",
       " 0.40602762,\n",
       " 0.40472835,\n",
       " 0.40343958,\n",
       " 0.40216115,\n",
       " 0.4008934,\n",
       " 0.3996373,\n",
       " 0.39839375,\n",
       " 0.39715987,\n",
       " 0.3959368,\n",
       " 0.39472494,\n",
       " 0.3935229,\n",
       " 0.3923295,\n",
       " 0.39114475,\n",
       " 0.3899682,\n",
       " 0.38879982,\n",
       " 0.38763943,\n",
       " 0.38648683]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce43f5afc3da1ea9c2859aca36b65d9af6136ef930ed7edf27ba0e49c79ddf9d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('pro1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
